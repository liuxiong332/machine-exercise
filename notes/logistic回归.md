# Logistic回归

当我们使用线性回归的时候，我们有$y = w_1x_1 + w_2x_2 + \cdots + w_dx_d$，但是我们可以把它应用在衍生模型中，即我们假设有$g(x)$使得$y = g(w_1x_1 + w_2x_2 + \cdots + w_dx_d)$ 。

特 别的，当我们在处理二分分类问题的时候，我们需要变量$x_i$产生输出为0和1的预测值，即我们需要
$$
g(z)=
\begin{cases}
0& z < 0  \\
0.5 & z = 0\\
1& z  > 0
\end{cases}
$$
其中，$z=w_1x_1 + w_2x_2 + \cdots + w_dx_d$即，我们需要这样的**阶跃函数**来产生二分预测值。

但是这样的阶跃函数在程序上很难实现，于是我们使用了如下**Sigmoid**函数：
$$
y = \frac{1}{1 + e^{-x}}
$$
此函数在$x = 0$的时候变化很陡，能近似的替代上述阶跃函数。其中z=w_1x_1 + w_2x_2 + \cdots + w_dx_d$。

为使得尽量多的点满足上述表达式，我们使用**最大似然估计**来估计w和b。

上述y是分布在$[0, 1]$区间，设$p(y=1|x) = \pi(x) =  \frac{1}{1 + e^{-wx-b}}$ ，则$p(y=0|x) = 1 - \pi(x) = \frac{1}{1 + e^{wx + b}}$

http://blog.csdn.net/ariessurfer/article/details/41310525