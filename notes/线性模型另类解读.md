# 线性模型-另类解读

##线性回归

对于d个属性描述的$(x_1, x_2, x_3, ..., x_d)$，我们可以使用线性模型来预测结果，即
$$
f(x) =w_{1}x_1 + x_2x_2 + ... + w_dx_d + b
$$
写成向量形式为$f(x) = \overrightarrow{w}^T\overrightarrow{x} + b$，其中$\overrightarrow{x} = [x_1, x_2, x_3, ... x_d]^T$ ，$\overrightarrow{w} = [w_1, w_2, ... w_d]^T$ 。

对于上述算式，我们需要找出$w_i$和$b$来使得$f(x)$和实际值$y_i$尽量接近。

我们设置$\epsilon = y  - f(x)$，即$\epsilon$表示真实值和预测值的误差，而我们知道$\epsilon$应该满足期望为0，方差为$\sigma^2$的正态分布

于是我们可以知道
$$
p(\epsilon) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{\epsilon^2}{2\sigma^2}) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y_i - w^Tx_i)^2}{2\sigma^2})
$$
我们目标是要使得$y_i - w^Tx_i$最小，也就是要使得$p(\epsilon)$最大，根据最大似然定律，我们就是要使得似然函数
$$
L(w) = \prod_{i=1}^{N}exp(-\frac{(y_i - w^Tx_i)^2}{2\sigma^2})
$$
最大，区似然函数的对数似然函数，我们可以得到如下：
$$
l(w) = lnL(w) = -\sum_{i=1}^{N}\frac{(y_i - w^Tx_i) ^ 2}{2\sigma ^ 2}
$$
要让上式最大，我们希望$y_i - w^Tx_i$最小，于是，我们根据**似然函数**得到的结论跟我们用**最小二乘法**得到的结论基本一样了哦。 

